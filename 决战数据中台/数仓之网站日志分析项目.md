# 这是我的第一个基于CDH搭建的网站日志分析离线数仓项目笔记

> 这个项目目标：
> 掌握离线数仓的整体流程
> 掌握搭建网站日志分析数仓方法
> 掌握对数据的初步清洗、对数仓表设计的各种方案、掌握初步的ETL经验
> 对基本的数仓工具做到熟练用于，会设计简单的数据报表

## CDH环境搭建（项目也单机版演示）

### 搭建流程

'''shell

# CHD6服务安装
## 所有节点都需要弄
1. 修改主机
2. 修改ip地址
3. 修改ip地址与主句的映射关系
4. 配置ssh免密登录
5. 关闭防火墙

## 关闭防火墙
systemctl stop firewalld

## 防火墙开机不启动
systemctl disable firewalld

6. 关闭SeLinux
vi /etc/selinux/config
# 将enforcing改为disabled

## 主节点安装
1. CM和CDH下载网站
[cm和cdh的资源网站](https://archive.cloudera.com/)

2. 下载cm6
[下载地址](https://archive.cloudera.com/cm6/6.2.1/redhat7/yum/RPMS/x86_64/)
全部下载

3. 下载cdh6
[下载地址](https://archive.cloudera.com/cdh6/6.2.1/parcels)
下载两个文件
把manifest.json里的内容放到一个自己创建的文件里
一共下载3个文件

4. 构建本地yum源
yum install -y httpd
service httpd start

cd /var/www/html/
mkdir cm6
mkdir cdh6

cd /opt/
cp *.rpm /var/www/html/cm6/
cp CDH-6.3.1.p0.17....parcel* /var/www/html/cdh6/
cd manifest.json /var/www/html/cdh6/

yum install -y createrepo
cd /var/www/html/cm6/
createrepo .


5. 在所有节点上添加yum源的配置文件

for i in {1..10}
do
ssh mingwang-$i '

cat >> /etc/yum.repos.d/cm6.repo << EOF
[cm6-local]
name=cm6-local
baseurl=http://mingwang-1/cm6
enabled=1
gpgcheck=0
EOF
exit'
done

## 查看yum源是存在
ssh mingwang-10
more /etc/yum.repos.d/cm6.repo

## 查看yum源是否生效
-- 看情况弄 yum clean all
yum repolist

6. 安装cm6和其他依赖
- 在所有节点上安装依赖
yum install -y bind-utils libxslt cyrus-sasl-plain cyrus-sasl-gssapi portmap fuse-libs /lib/lsb/init-functions httpd mod_ssl openssl-devel python-psycopg2 MySQL-python fuse

- 安装在管理节点
yum install -y oracle-j2sdk1.8-1.8.0+update181-1.x86_64
yum install -y cloudera-manager-daemons cloudera-manager-server cloudera-manager-server-db-2 postgresql-server

7. 安装mysql
注意：CentOS7 yum源不再安装默认的mysql了，而是使用MariaDB
- 安装MariaDB
yum install -y mysql mysql-devel
yum install -y mariadb mariadb-server

- 初始化MariaDB并修改密码
service mariadb start
chkconfig mariadb on
/usr/bin/mysql_secure_installation

## 第一个问你是数据库原密码，第一次可以直接回车
## 第二个问你是不是设置root密码，回车设置
## 第三个问你是不是删除匿名用户，是的
## 第四个问你是不是禁止root用户远程登录
## 第五个问你是不是删除测试数据库，是的
## 第六个问你是不是刷新旧配置
## 到此安装完成
## 高可用学完再说--------------------------------》》》》》》》

- 注意：由于数据库存储的信息非常重要，所以生成环境建议配置成高可用的集群。

8. 初始化管理节点数据库（所有节点都分发一下mysql驱动）

- 拷贝mysql-connector到/usr/share/java目录，并且改名为mysql-connector-java.jar
mkdir -p /usr/share/java/
cp /opt/mysql-connector-java-5.1.47.jar /usr/share/java/mysql-connector-java.jar
## 执行数据库初始脚本
/opt/cloudera/cm/schema/scm_prepare_database.sh mysql -h localhost -uroot -ptoor --scm-host localhost scm root toor

9. 安装agent节点
- 在所有的子节点上安装
yum install -y oracle-j2sdk1.8-1.8.0+update181-1.x86_64
yum install -y cloudera-manager-daemons cloudera-manager-agent

- 修改所有节点的配置文件，让agent的地址指向cloudera-manager-server
sed -i "s/server_host=localhost/server_host=mingwang-1/g" /etc/cloudera-scm-agent/config.ini

10. 启动server
## systemctl start cloudera-scm-server
## systemctl enable cloudera-scm-server

service cloudera-scm-server start
service cloudera-scm-server on

11. 启动agent
## 所有节点都启动
### systemctl start cloudera-scm-agent
### systemctl enable cloudera-scm-agent

service cloudera-scm-agent start
chkconfig cloudera-scm-agent on

## 登录管理界面

1. 配置win上的host映射
2. 访问server服务：mingwang-1:7180
3. 使用admin登录后先改管理密码
4. 下一步，我们可以先使用60天试用版，60天后会自动降到免费版
5. 给集群起个名字
6. 选择我们事先准备好的parcel包，点击它后面的-更多选项-删除多有的链接-修改成我们自己的--http://mingwang-1/cdh6 -- 看到我们自己准备的后选择
7. 把出现的问题进行相应的修改

## 修改Cloudera警告
for i in {1...10}
do
    ssh mingwang-$i '
        echo 10 > /proc/sys/vm/swappiness
    exit'
done

## 关闭透明大页
1. 修改警告
echo never > /sys/kernel/mm/transparent_hugepage/defrag
echo never > /sys/kernel/mm/transparent_hugepage/enabled

2. 添加到启动项
echo "echo never > /sys/kernel/mm/transparent_hugepage/defrag" >> /etc/rc.local
echo "echo never > /sys/kernel/mm/transparent_hugepage/enabled" >> /etc/rc.local

## 批量修改(用这个)
for i in {1...10}
do
    ssh node-$i '
        echo never > /sys/kernel/mm/transparent_hugepage/defrag
        echo never > /sys/kernel/mm/transparent_hugepage/enabled
        echo "echo never > /sys/kernel/mm/transparent_hugepage/defrag" >> /etc/rc.local
        echo "echo never > /sys/kernel/mm/transparent_hugepage/enabled" >> /etc/rc.local
    exit'
done

## 查看是否修改成功
more /sys/kernel/mm/transparent_hugepage/defrag

## 安装服务

> 安装的时候需要注意先后顺序---把最下面的监控服务装上
> 监控服务装到第一台上

## 创建普通用户
create user gz identified by 'gz';

## 建库
### 创建信息上报的数据库
create database reports default charset utf8;

### 创建活动监控的数据库
create database activity default charset utf8;
create database audit default charset utf8;
create database metadata default charset utf8;

## 允许mysql远程登录
### 赋予权限
grant all privileges on reports.* to gz@'%' identified by 'gz';
grant all privileges on activity.* to gz@'%' identified by 'gz';
grant all privileges on audit.* to gz@'%' identified by 'gz';
grant all privileges on metadata.* to gz@'%' identified by 'gz';
flush privileges;
'''

### 服务安装流程--------------------------------------------》》》》》

## 前端埋点 & 后端的业务对接（了解）

'''js
# 在你想要埋点的地方放入以下代码
// 埋点
      var _maq = _maq || [];
        _maq.push({'_setAccount':'website'});

        (function () {
            var ma = document.createElement("script");
            ma.type = "text/javascript";
            ma.async = true;
            // a ? 0 : 1
            ma.src = ("https:" == document.location.protocol?
                'https://mingwang-1:80/static/js/ga.js' : 'http://mingwang-1:80/static/js/ga.js');
            // document.getElementById
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ma, s);
        })();

# 和nginx交互的文件 ga.js
(function () {

    var params = {};

    //Document对象数据
    if (document) {
        params.domain = document.domain || '';
        params.url = document.URL || '';
        params.title = document.title || '';
        params.referrer = document.referrer || '';

    }

    //Window对象数据
    if (window && window.screen) {
        params.sh = window.screen.height || 0;
        params.sw = window.screen.width || 0;
        params.cd = window.screen.colorDepth || 0;
    }

    //navigator对象数据
    if (navigator) {
        params.lang = navigator.language || '';
    }

    //解析_maq配置
    if (_maq) {
        for (var i in _maq) {
            switch (_maq[i][0]) {
                case '_setAccount':
                    params.account = _maq[i][1];
                    break;
                default:
                    break;
            }
        }
    }

    //拼接参数串
    var args = '';
    for (var i in params) {
        if (args != '') {
            args += '&';
        }

        // args += i + '=' + encodeURIComponent(params[i]);
        args += i + '=' + params[i];

    }

    //通过Image对象请求后端脚本
    var img = new Image(1, 1);
    img.src = 'http://mingwang-1:81/_utm.gif?' + args;

})();

'''

## 大数据单机基础服务搭建

hadoop是我们使用以下服务的基础，所以我们要搭建一个简单的单机伪分布式机器

### 环境变量-我们可以提前准备好
export JAVA_HOME=/opt/jdk
export PATH=:$JAVA_HOME/bin:$PATH

export TOMCAT_HOME=/opt/tomcat
export PATH=:$TOMCAT_HOME/bin:$PATH

export HADOOP_HOME=/opt/hadoop
export PATH=:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH

export HIVE_HOME=/opt/hive
export PATH=:$HIVE_HOME/bin:$PATH

export SQOOP_HOME=/opt/sqoop
export PATH=:$SQOOP_HOME/bin:$PATH

export FLUME_HOME=/opt/flume
export PATH=:$FLUME_HOME/bin:$PATH

### 搭建流程

'''shell

1. mapred-site.xml  可能没有，但是有一个模板文件（执行命令：cp mapred-site.xml.template mapred-site.xml）
<configuration>
        <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
        </property>
</configuration>

2. core-site.xml          mingwang-1是我的主机名，我在/etc/hosts里做了映射，这里可以换成本机ip或127.0.0.1
<configuration>
 <property>
   <name>fs.default.name</name>
   <value>hdfs://mingwang-1:8020</value>
 </property>
</configuration>

3. hdfs-site.xml    指定副本数、namenode和datanode的存储位置
<configuration>
 <property>
   <name>dfs.replication</name>
   <value>1</value>
 </property>
  <property>
     <name>dfs.namenode.name.dir</name>
     <value>/home/hadoop/dfs/name</value>
  </property>
  <property>
     <name>dfs.datanode.data.dir</name>
     <value>/home/hadoop/dfs/data</value>
  </property>
</configuration>

4. yarn-site.xml 
<configuration>
 <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
 </property>
</configuration>

5. slaves 这个是写datanode的ip地址的，由于是单节点的，所以写本机就行了
localhost

6. hdfs-site.xml  修改一下java的家目录，和hadoop的配文件目录

'''

### 使用

'''shell
启动
hadoop namenode -format
hadoop-daemon.sh start namenode
hadoop-daemon.sh start datanode
start-yarn.sh
'''

## 接收埋点日志数据

采用nginx技术接收，下面是安装与使用方法：

### 安装

'''shell
# nginx安装笔记

## 安装依赖
yum -y install gcc-c++ pcre pcre-devel zlib zlib-devel openssl openssl-devel wget

## 下载源码包
[官网](https://nginx.org/en/download.html)
### 推荐下载命令
wget -c https://nginx.org/download/nginx-1.10.1.tar.gz

### 减压
tar -zxvf nginx-1.10.1.tar.gz
cd nginx-1.10.1

### 使用默认配置即可
./configure

### 编译安装
make
make install

### 查找安装路径
whereis nginx

## 启动、关闭nginx
cd /usr/local/nginx/sbin/
./nginx
./nginx -s stop
./nginx -s quit
./nginx -s reload

> ./nginx -s quit:此方式停止步骤是待nginx进程处理任务完毕进行停止。
> ./nginx -s stop:此方式相当于先查出nginx进程id再使用kill命令强制杀掉进程。

## 查询nginx进程：
ps aux|grep nginx

## 重启 nginx
### 1.先停止再启动（推荐）：
对 nginx 进行重启相当于先停止再启动，即先执行停止命令再执行启动命令。如下：
./nginx -s quit
./nginx

### 2.重新加载配置文件：
当 ngin x的配置文件 nginx.conf 修改后，要想让配置生效需要重启 nginx，使用-s reload不用先停止 ngin x再启动 nginx 即可将配置信息在 nginx 中生效，如下：
./nginx -s reload

启动成功后，在浏览器可以看到页面

## 开机自启动
即在rc.local增加启动代码就可以了。
vi /etc/rc.local

增加一行： `/usr/local/nginx/sbin/nginx`
设置执行权限：
chmod 755 rc.local

'''

### 使用

修改如下配置即可，个人推荐在第一次启动的时候先把flume启动后在回来启动nginx
'''shell
【这里我把代理的这段去打掉了】
 【"$http_x_forwarded_for"】
 
    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent"';

access_log  /usr/local/nginx/logs/log1/access.log  main;

        listen       81;
'''

## 获取全量数据

采用flume技术，下面是安装与使用的方法：

### 安装

'''shell

## flume
cd /opt/flume/conf
cp flume-env.sh.template flume-env.sh
vim flume-env.sh
#增加JAVA_HOME
export JAVA_HOME=/opt/jdk
执行的文件自己配置
我把他们放到/opt/datas/flume 里了
自定义文件aa.conf:
a1.sources = s1
a1.channels = c1
a1.sinks = sk1

a1.sources.s1.type = TAILDIR
a1.sources.s1.positionFile = /opt/datas/flume/json/taildir_position.json
a1.sources.s1.filegroups = f1 f2
a1.sources.s1.filegroups.f1 = /usr/local/nginx/logs/log1/access.log
a1.sources.s1.filegroups.f2 = /usr/local/nginx/logs/log2/.*.log.*

a1.sinks.sk1.type = hdfs
a1.sinks.sk1.hdfs.path=hdfs://mingwang-1:8020/flume/%y-%m-%d/%H-%M/
a1.sinks.sk1.hdfs.filePrefix= baidu-
a1.sinks.sk1.hdfs.round = true
a1.sinks.sk1.hdfs.roundValue = 10
a1.sinks.sk1.hdfs.roundUnit = minute
a1.sinks.sk1.hdfs.rollInterval= 0
a1.sinks.sk1.hdfs.rollSize= 134217728
a1.sinks.sk1.hdfs.rollCount= 0
a1.sinks.sk1.hdfs.batchSize = 1
a1.sinks.sk1.hdfs.idleTimeout= 60
a1.sinks.sk1.hdfs.useLocalTimeStamp = true
a1.sinks.sk1.hdfs.fileType = DataStream

a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

a1.sources.s1.channels = c1
a1.sinks.sk1.channel = c1

'''

### 使用

个人推荐在第一次启动的时候要在nginx之前启动，同时flume极其容易掉，所以尽可能少的写拦截器，可以采用hadoop的MR对数据进行初步的处理，应为它的并行的，可以在所有装有hadoop服务的机器上运行。

'''shell

简单运行
flume-ng agent -n a1 -c conf -f /opt/datas/flume/aa.conf

前台运行
flume-ng agent -n a1 -c conf -f /opt/datas/flume/aa.conf -Dflume.root.logger=INFO,console

flume后台运行
flume-ng agent -n a1 -c conf -f /opt/datas/flume/aa.conf -Dflume.root.logger=INFO,console &

'''

## 对日志进行简单的处理 ----------------------------------------------------》》》》》》正在写

我们采用hadoop的MR进行处理，具体流程如下：

### 日志分析

'''shell
1. 我们在对日志进行简单处理的时候，不需要聚合操作，所以不会后sufffer，执行效率和flume的拦截器不相上下，反而因为简化了拦截器的能耗，一定程度上降低了flume掉线的概率，同时可以提高flume的复用。
此时我们可以通过 job.setNumRedutetasks(0); 来达到不适用Reduce的功能。


'''

### 处理代码

'''java
package com.mingwang.preprocess;

import org.apache.hadoop.io.Writable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

/**
 * 对接外部数据的层，表结构最好和外部数据保持一致
 * 术语：贴源表
 */
public class WeblogBean implements Writable {

    private boolean volid = true;   //判断数据是否合法
    private String remote_addr; //记录客户端的ip地址
    private String remote_user; //记录客户端用户名称，忽略属性 "-"
    private String time_local;  //记录访问时间与时区
    private String request; //记录请求的 url 和请求的 http 协议
    private String status;  //记录请求状态：成功是200
    private String body_bytes_sent; //记录发送给客户端文件主体内容的大小
    private String http_referer;    //用来记录从那个页面链接访问过来的
    private String http_user_agent; //记录客户浏览器的相关信息

    public void set(boolean volid, String remote_addr, String remote_user, String time_local, String request, String status, String body_bytes_sent, String http_referer, String http_user_agent) {
        this.volid = volid;
        this.remote_addr = remote_addr;
        this.remote_user = remote_user;
        this.time_local = time_local;
        this.request = request;
        this.status = status;
        this.body_bytes_sent = body_bytes_sent;
        this.http_referer = http_referer;
        this.http_user_agent = http_user_agent;
    }

    public boolean isVolid() {
        return volid;
    }

    public void setVolid(boolean volid) {
        this.volid = volid;
    }

    public String getRemote_addr() {
        return remote_addr;
    }

    public void setRemote_addr(String remote_addr) {
        this.remote_addr = remote_addr;
    }

    public String getRemote_user() {
        return remote_user;
    }

    public void setRemote_user(String remote_user) {
        this.remote_user = remote_user;
    }

    public String getTime_local() {
        return time_local;
    }

    public void setTime_local(String time_local) {
        this.time_local = time_local;
    }

    public String getRequest() {
        return request;
    }

    public void setRequest(String request) {
        this.request = request;
    }

    public String getStatus() {
        return status;
    }

    public void setStatus(String status) {
        this.status = status;
    }

    public String getBody_bytes_sent() {
        return body_bytes_sent;
    }

    public void setBody_bytes_sent(String body_bytes_sent) {
        this.body_bytes_sent = body_bytes_sent;
    }

    public String getHttp_referer() {
        return http_referer;
    }

    public void setHttp_referer(String http_referer) {
        this.http_referer = http_referer;
    }

    public String getHttp_user_agent() {
        return http_user_agent;
    }

    public void setHttp_user_agent(String http_user_agent) {
        this.http_user_agent = http_user_agent;
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append(this.volid);
        sb.append("\001").append(this.getRemote_addr());
        sb.append("\001").append(this.getRemote_user());
        sb.append("\001").append(this.getTime_local());
        sb.append("\001").append(this.getRequest());
        sb.append("\001").append(this.getStatus());
        sb.append("\001").append(this.getBody_bytes_sent());
        sb.append("\001").append(this.getHttp_referer());
        sb.append("\001").append(this.getHttp_user_agent());
        return sb.toString();
    }

    @Override
    public void write(DataOutput out) throws IOException {
        out.writeBoolean(this.volid);
        out.writeUTF(null == remote_addr ? "" : remote_addr);
        out.writeUTF(null == remote_user ? "" : remote_user);
        out.writeUTF(null == time_local ? "" : time_local);
        out.writeUTF(null == request ? "" : request);
        out.writeUTF(null == status ? "" : status);
        out.writeUTF(null == body_bytes_sent ? "" : body_bytes_sent);
        out.writeUTF(null == http_referer ? "" : http_referer);
        out.writeUTF(null == http_user_agent ? "" : http_user_agent);
    }

    @Override
    public void readFields(DataInput in) throws IOException {
        this.volid = in.readBoolean();
        this.remote_addr = in.readUTF();
        this.remote_user = in.readUTF();
        this.time_local = in.readUTF();
        this.request = in.readUTF();
        this.status = in.readUTF();
        this.body_bytes_sent = in.readUTF();
        this.http_referer = in.readUTF();
        this.http_user_agent = in.readUTF();
    }
}
'''

'''java
package com.mingwang.preprocess;

import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Locale;
import java.util.Set;
import java.util.regex.Pattern;

public class WeblogParser {
    public static SimpleDateFormat df1 = new SimpleDateFormat("d/MMM/yyyy:HH:mm:ss",Locale.US);
    public static SimpleDateFormat df2 = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss",Locale.US);

    public static WeblogBean parser(String line) {
        WeblogBean weblogBean = new WeblogBean();

        // 按照行容分割符进行数据切割
        String[] arr = line.split(" ");
        if (line.length() > 11) {
            weblogBean.setRemote_addr(arr[0]);
            weblogBean.setRemote_user(arr[1]);
            
            String time_local = formatDate(arr[3].substring(1));
            if (null == time_local || "".equals(time_local)) time_local = "-invalid_time-";
            weblogBean.setTime_local(time_local);

            //拆分字符串request
            Pattern p1 = Pattern.compile("( |\\?domain=|&url=|&title=|&referrer=&sh=|&sw=|&cd=|&lang=)");
            String[] parts = p1.split(arr[6]);
            String request = "";
            for (int i = 0; i < parts.length; i++) {
                if (parts.length -1 != i) {
                    request += parts[i] + "\001";
                } else {
                    request += parts[i];
                }
            }
            weblogBean.setRequest(request);   // 如何拆分字符串
            
            weblogBean.setStatus(arr[8]);
            weblogBean.setBody_bytes_sent(arr[9]);
            weblogBean.setHttp_referer(arr[10]);

            // 如果useragent元素较多，拼接useragent
            if (arr.length > 12) {
                StringBuilder sb = new StringBuilder();
                for(int i = 11; i < arr.length; i++) {
                    sb.append(arr[i]);
                }
                weblogBean.setHttp_user_agent(sb.toString());
            } else {
                weblogBean.setHttp_user_agent(arr[11]);
            }

            if (Integer.parseInt(weblogBean.getStatus()) >= 400) {  //大于400，HTTP错误
                weblogBean.setVolid(false);
            }

            if ("-invalid_time-".equals(weblogBean.getTime_local())) {
                weblogBean.setVolid(false);
            }
        } else {
            weblogBean = null;
        }
        return weblogBean;
    }

    /**
     * 格式化时间方法
     * @return
     */
    public static String formatDate(String time_local) {
        try {
            return df2.format(df1.parse(time_local));
        } catch (ParseException e) {
            return null;
        }
    }

    public static void filtStaticResource(WeblogBean bean, Set<String> pages) {
        if (!pages.contains(bean.getRequest())) {
            bean.setVolid(false);
        }
    }
}
'''

'''java
package com.mingwang.preprocess;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;
import java.util.HashSet;
import java.util.Set;

/**
 * 处理原始数据，过滤出真实的pv请求 转换时间格式 对缺失字段填充默认值，对标记的valid和inavlid
 */
public class WeblogPreProcess {
    static class WeblogProProcessMapper extends Mapper<LongWritable, Text, WeblogBean, NullWritable> {
        // 用来存储网站url分类数据
        Set<String> pages = new HashSet<>();
        NullWritable v = NullWritable.get();

        /**
         * 从外部配置文件中加载网站有用的url分类数据，存储在maptask的内存中，用来对日志数据进行过滤,根据公司的实际业务设置
         * @param context
         * @throws IOException
         * @throws InterruptedException
         */
        @Override
        protected void setup(Context context) throws IOException, InterruptedException {
            pages.add("/CourseController");
            pages.add("/findAllImg");
            pages.add("/allCourse");
            pages.add("/addCourse");
            pages.add("/deleteCourse/*");
            pages.add("/findPageObject");
            pages.add("/CourseMore");
            pages.add("/CourseRA");
            pages.add("/selectNew");
            pages.add("/courseNameLike");
            pages.add("/FileController");
            pages.add("/layuiFileUpload");
            pages.add("/NavController");
            pages.add("/findDetails");
            pages.add("/navMore");
            pages.add("/userController");
            pages.add("/findAllUser");
            pages.add("/findUserById");
            pages.add("/updatePwd");
            pages.add("/login");
            pages.add("/registered");
            pages.add("/denglu");
            pages.add("/updateUserName");
            pages.add("/findUserByID");
            pages.add("/svip");
        }

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString();

            WeblogBean weblogBean = WeblogParser.parser(line);

            if (weblogBean != null) {
                // 过滤js/图片/css等静态资源
                WeblogParser.filtStaticResource(weblogBean, pages);
                context.write(weblogBean, v);
            }
        }
    }

    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        Job job = Job.getInstance(new Configuration());
        job.setJarByClass(WeblogPreProcess.class);
        job.setMapperClass(WeblogProProcessMapper.class);

        job.setOutputKeyClass(WeblogBean.class);
        job.setOutputValueClass(NullWritable.class);

        FileInputFormat.setInputPaths(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
//        FileInputFormat.setInputPaths(job, new Path("C:\\Users\\26330\\Downloads\\intput"));
//        FileOutputFormat.setOutputPath(job, new Path("C:\\Users\\26330\\Downloads\\output"));

        job.setNumReduceTasks(0);
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
'''

### 使用方法

'''shell
hadoop jar web-etl.jar com.mingwang.preprocess.WeblogPreProcess /flume-etl/20-12-07/04-50 /flume/log
'''

## 数据入库

这里我们的入库操作，是hive开展的。
当我们对数据进行简单的拆分后，就需要根据领导下发的主题使用我们所领会的经验进行相关表的设计，心中要有一个整体的架构图具体操作如下：

### 安装

'''shell

1. hive-env.sh
cp hive-env.sh.template hive-env.sh
 
2. hadoop 目录-相应的目录换成自己的
HADOOP_HOME=/opt/hadoop

3. hive 配置目录
export HIVE_CONF_DIR=/opt/hive/conf

4. hive 的lib目录
export HIVE_AUX_JARS_PATH=/opt/hive/lib

5. hive-site.xml
<configuration>
     <!-- jdbc-url -->
     <property>
         <name>javax.jdo.option.ConnectionURL</name>
         <value>jdbc:mysql://mingwang-1:3306/hive?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
     </property>
     <!-- jdbc-driver -->
     <property>
         <name>javax.jdo.option.ConnectionDriverName</name>
         <value>com.mysql.jdbc.Driver</value>
     </property>
     <!-- 数据库用户名 -->
     <property>
         <name>javax.jdo.option.ConnectionUserName</name>
         <value>root</value>
     </property>
     <!-- 数据密码-->
     <property>
         <name>javax.jdo.option.ConnectionPassword</name>
         <value>root</value>
     </property>
     <property>
         <name>datanucleus.readOnlyDatastore</name>
         <value>false</value>
     </property>
     <property>
         <name>datanucleus.fixedDatastore</name>
         <value>false</value>
     </property>
     <property>
         <name>datanucleus.autoCreateSchema</name>
         <value>true</value>
     </property>
     <property>
         <name>datanucleus.schema.autoCreateAll</name>
         <value>true</value>
     </property>
     <property>
         <name>datanucleus.autoCreateTables</name>
         <value>true</value>
     </property>
     <property>
         <name>datanucleus.autoCreateColumns</name>
         <value>true</value>
     </property>
     <property>
         <name>hive.metastore.local</name>
         <value>true</value>
     </property>
    <!--默认为/user/hive/warehouse,用于配置Hive默认的数据文件存储路径，这是一个HDFS路径。可按需调整。-->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/hive/warehouse</value>
    </property>
    <!--查询输出是是否打印名字和列，默认是false-->
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>
    <!--hive的提示里是否包含当前的db，默认为false-->
    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>
</configuration>

'''

### 使用

'''shell

1. 格式化数据库
schematool -dbType mysql -initSchema
 
2. 启动hive
hive
 
3. 查看数据库
hive> show databases;

'''

#### 设计表

创建数据库
'''sql
create database k18etl;
use k18etl;
'''

原始数据表，对应清洗完之后的数据，而不是原始日志数据。
'''sql
drop table if exists ods_weblog_origin;
create table ods_weblog_origin (
valid string,
remote_addr string,
remote_user string,
time_local string,
request string,
status string,
body_bytes_sent string,
http_referer string,
http_user_agent string)
partitioned by (datestr string)
row format delimited
fields terminated by '\001';
'''

点击流表pageview表
'''sql
drop table if exists ods_click_pageviews;
create table ods_click_pageviews (
session string,
remote_addr string,
remote_user string,
time_local string,
request string,
visit_step string,
page_staylong string,
http_referer string,
http_user_agent,
body_bytes_sent string,
status string)
partitioned by (datestr string)
row format delimited
fields terminated by '\001';
'''

点击流表visit表
'''sql
drop table if exists ods_click_stream_visit;
create table ods_click_stream_visit (
session string,
remote_addr string,
inTime string,
inPage string,
outPage string,
referal string,
pageVisits int)
partitioned by (datestr string)
row format delimited
fields terminated by '\001';
'''

维度表示例：
'''sql
drop table if exists t_dim_time;
cteate table t_dim_time(
date_key int,
year string,
month string,
day string,
hour string)
row format delimited
fields terminated by ',';
'''

#### 数据导入

把mysql驱动弄好：
mkdir -p /usr/share/java/
cp /opt/mysql-connector-java-5.1.47.jar /usr/share/java/mysql-connector-java.jar

导入清洗结果数据到贴源（和源数据几乎相似）数据表 ods_weblog_origin
'''sql
load data inpath '/flume/log' overwrite into table ods_weblog_origin partition(datestr='20201207');
show partitions ods_weblog_origin;
select count(*) from ods_weblog_origin;
'''

## ETL操作

### 设计表

### 数据导入

### 细化操作（这里是我们今后需要关注的重点之一）

## 数据出库

数据出库我们使用到的技术是sqoop、kettle，可以根据公司的实际情况选择，其次可以根据个人的使用习惯来定。

### 安装

'''shell

mv sqoop-env-template.sh sqoop-env.sh
vim sqoop-env.sh

export HADOOP_COMMON_HOME=/opt/hadoop
export HADOOP_MAPRED_HOME=/opt/hadoop
export HIVE_HOME=/opt/hive
export HIVE_CONF_DIR=/opt/hive/conf

如果报zookeeper的错误把他们注释了就可以了 vim configure-sqoop

--------------------测试连通性-----------------------
sqoop list-tables -connect jdbc:mysql://mingwang-1:3306/hive --username root --password root

--------------------问题解决-----------------------
将hive的lib里面的拷贝到sqoop的lib目录下
cp hive-exec-1.2.1.jar /usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib/

--------------------找不到主类-----------------
cd /opt/sqoop/lib目录下
cp sqoop-1.4.4.jar /opt/hadoop/share/hadoop/mapreduce/lib/

----------------------报错解决-----------------------------
cp /opt/hive/lib/hive-shims-* /opt/sqoop/lib/

----------------------测试数据库到hive-----------------
编写脚本文件 vi conf1

'''

### 使用

'''shell

----------------------测试数据库到hive-----------------
脚本指定的方式编写脚本文件 vi conf1
import
--connect
jdbc:mysql://mingwang-1:3306/gz
--username
root
--password
root
--table
city
--columns
cid,cname,pid
--where
id>0
--target-dir
hdfs://mycluster/sqoop
--delete-target-dir
-m
1
--as-textfile

--hive-import
--hive-overwrite
--create-hive-table
--hive-table
city
---------------执行sqoop脚本--------------------
sqoop --options-file conf1

-------------------hive到mysql------------------------
需要在mysql上把表创建好
导出数据：
sqoop export \
--connect jdbc:mysql://mingwang-1:3306/gz?characterEncoding=UTF-8 \
--username root \
--password root \
--table nihao \
--export-dir '/user/hive/warehouse/gz.db/nihao' \
--input-null-string 'NaN' \
--input-null-non-string 'NaN' \
--fields-terminated-by '\001'
--batch

# 增量导出
sqoop export \
--connect jdbc:mysql://mingwang-1:3306/gz?characterEncoding=UTF-8 \
--username root \
--password root \
--table nihao \
--export-dir '/user/hive/warehouse/gz.db/nihao' \
--input-null-string 'NaN' \
--input-null-non-string 'NaN' \
--fields-terminated-by '\001' \
--update-mode allowinsert \
--update-key id \
--batch

'''

## 任务调度

任务调度的方案有很多，可以更具公司的需求选择，这里我们选择与CDH整合在一起的oozie。

### 使用

### 脚本的编写

## 数据展示平台的搭建与设计（了解即可）

报表我们采用Echarts来开展，具体方法如下：

### 前端文件的准备

### 后端数据与前端的动态交互

数据动态交互我们采用SSM框架来完成，具体如下






































