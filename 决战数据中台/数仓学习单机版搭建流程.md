# hadoop，flume（1.9.0支持它TAILDIR），hive，sqoop

## 环境变量
export JAVA_HOME=/opt/jdk
export PATH=:$JAVA_HOME/bin:$PATH

export TOMCAT_HOME=/opt/tomcat
export PATH=:$TOMCAT_HOME/bin:$PATH

export HADOOP_HOME=/opt/hadoop
export PATH=:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH

export HIVE_HOME=/opt/hive
export PATH=:$HIVE_HOME/bin:$PATH

export SQOOP_HOME=/opt/sqoop
export PATH=:$SQOOP_HOME/bin:$PATH

export FLUME_HOME=/opt/flume
export PATH=:$FLUME_HOME/bin:$PATH

## mysql
1.rpm 查看 mariadb，是否存在，如果存在必须卸载
rpm -qa|grep mariadb
        如: mariadb-libs-5.5.65-1.el7.x86_64
2.卸载 mariadb
		rpm -e --nodeps	mariadb-libs-5.5.65-1.el7.x86_64
3. 为了避免出现权限问题，给 mysql 解压文件所在目录赋予最大权限
		在 opt 下创建 mysql 解压目录
			mkdir mysql
		设置 mysql 解压目录的权限
			chmod -R 777 mysql
        安装 MySQL 需要的一些依赖程序
			yum -y install make gcc-c++ cmake bison-devel ncurses-devel libaio libaio-devel net-tools

4. 解压 mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar 到/opt/mysql
		tar -xvf mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar -C /opt/mysql/
5. 严格按照顺序安装：
		// mysql-community-common-5.7.29-1.el7.x86_64.rpm、
		// mysql-community-libs-5.7.29-1.el7.x86_64.rpm、
		// mysql-community-client-5.7.29-1.el7.x86_64.rpm、
		// mysql-community-server-5.7.29-1.el7.x86_64.rpm这四个包
		// cd 到 mysql 目录下，依次输入命令
        centos7：
             rpm -ivh mysql-community-common-5.7.29-1.el7.x86_64.rpm
			 rpm -ivh mysql-community-libs-5.7.29-1.el7.x86_64.rpm
			 rpm -ivh mysql-community-client-5.7.29-1.el7.x86_64.rpm
			 rpm -ivh mysql-community-server-5.7.29-1.el7.x86_64.rpm
        centos6：
			 rpm -ivh mysql-community-common-5.7.29-1.el7.x86_64.rpm
			 rpm -ivh mysql-community-libs-5.7.29-1.el7.x86_64.rpm --force --nodeps
			 rpm -ivh mysql-community-client-5.7.29-1.el7.x86_64.rpm --force --nodeps
			 rpm -ivh mysql-community-server-5.7.29-1.el7.x86_64.rpm --force --nodeps
6. 配置数据库
		vim /etc/my.cnf
		// 在 [mysqld] 下添加这三行
skip-grant-tables
character_set_server=utf8
init_connect='SET NAMES utf8'
		// skip-grant-tables：跳过登录验证
		// character_set_server=utf8：设置默认字符集UTF-8
		// init_connect='SET NAMES utf8'：设置默认字符集UTF-8
7. 启动服务
		systemctl start mysqld.service #启动服务
		systemctl disable mysql.service #关闭服务
8. 启动 mysql
		输入 mysql
9. 先设置一个简单的临时密码
		update mysql.user set authentication_string=password('root') where user='root';
		// 立即生效
			flush privileges;
10. 退出（exit）mysql 再次登录
		mysql -uroot -proot
		// 重设密码
			set password=password('root');
		// 立即生效
			flush privileges;
11. 进入 mysql
		mysql -uroot -proot
		// 赋值权限
			alter user user() identified by "root";
		// 刷新
			flush privileges;
12. 使用 MySQL 数据库
		use mysql;
		// 查询 user 表
			select User,Host from user;
13. 修改 user 表，把 Host 表内容修改为 %
		update user set host='%' where host='localhost';
14. 删除 root 用户的其他 host
delete from user where Host='mingwang-1';
delete from user where Host='127.0.0.1';
delete from user where Host='::1';
// 刷新
GRANT ALL PRIVILEGES ON *.* TO root@"%" IDENTIFIED BY "root"; 
	flush privileges;
// 查询 user 表修改之后的结果
	select User,Host from user;
// 退出

## hadoop
vim hadoop-env.sh  修改里面的jdk环境变量，与hadoop的配置文件

mapred-site.xml  可能没有，但是有一个模板文件（执行命令：cp mapred-site.xml.template mapred-site.xml）
<configuration>
        <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
        </property>
</configuration>

core-site.xml          mingwang-1是我的主机名，我在/etc/hosts里做了映射，这里可以换成本机ip或127.0.0.1
<configuration>
 <property>
   <name>fs.default.name</name>
   <value>hdfs://mingwang-1:8020</value>
 </property>
</configuration>

hdfs-site.xml    指定副本数、namenode和datanode的存储位置
<configuration>
 <property>
   <name>dfs.replication</name>
   <value>1</value>
 </property>
  <property>
     <name>dfs.namenode.name.dir</name>
     <value>/home/hadoop/dfs/name</value>
  </property>
  <property>
     <name>dfs.datanode.data.dir</name>
     <value>/home/hadoop/dfs/data</value>
  </property>
</configuration>

yarn-site.xml 
<configuration>
 <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
 </property>
</configuration>

slaves 这个是写datanode的ip地址的，由于是单节点的，所以写本机就行了
localhost

hdfs-site.xml  修改一下java的家目录，和hadoop的配文件目录

### 启动
hadoop namenode -format
hadoop-daemon.sh start namenode
hadoop-daemon.sh start datanode
start-yarn.sh

## hive

hive-env.sh
cp hive-env.sh.template hive-env.sh
# 相应的目录换成自己的
# hadoop 目录
HADOOP_HOME=/opt/hadoop

# hive 配置目录
export HIVE_CONF_DIR=/opt/hive/conf

# hive 的lib目录
export HIVE_AUX_JARS_PATH=/opt/hive/lib

hive-site.xml
<configuration>
     <!-- jdbc-url -->
     <property>
         <name>javax.jdo.option.ConnectionURL</name>
         <value>jdbc:mysql://mingwang-1:3306/hive?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
     </property>
     <!-- jdbc-driver -->
     <property>
         <name>javax.jdo.option.ConnectionDriverName</name>
         <value>com.mysql.jdbc.Driver</value>
     </property>
     <!-- 数据库用户名 -->
     <property>
         <name>javax.jdo.option.ConnectionUserName</name>
         <value>root</value>
     </property>
     <!-- 数据密码-->
     <property>
         <name>javax.jdo.option.ConnectionPassword</name>
         <value>root</value>
     </property>
     <property>
         <name>datanucleus.readOnlyDatastore</name>
         <value>false</value>
     </property>
     <property>
         <name>datanucleus.fixedDatastore</name>
         <value>false</value>
     </property>
     <property>
         <name>datanucleus.autoCreateSchema</name>
         <value>true</value>
     </property>
     <property>
         <name>datanucleus.schema.autoCreateAll</name>
         <value>true</value>
     </property>
     <property>
         <name>datanucleus.autoCreateTables</name>
         <value>true</value>
     </property>
     <property>
         <name>datanucleus.autoCreateColumns</name>
         <value>true</value>
     </property>
     <property>
         <name>hive.metastore.local</name>
         <value>true</value>
     </property>
     <!-- 显示表的列名 -->
     <property>
         <name>hive.cli.print.header</name>
         <value>true</value>
     </property>
     <!-- 显示数据库名称 -->
     <property>
         <name>hive.cli.print.current.db</name>
         <value>true</value>
     </property>
     <property>
         <name>hive.exec.local.scratchdir</name>
         <value>/usr/local/hive</value>
         <description>Local scratch space for Hive jobs</description>
     </property>
</configuration>

# 把mysql的驱动放到hive的lib目录下

# 格式化数据库
 schematool -dbType mysql -initSchema
 
 # 启动hive
 hive
 
 # 查看数据库
 hive> show databases;

## flume
cd /opt/flume/conf
cp flume-env.sh.template flume-env.sh
vim flume-env.sh
#增加JAVA_HOME
export JAVA_HOME=/opt/jdk
执行的文件自己配置
我把他们放到/opt/datas/flume 里了
'''conf
a1.sources = s1
a1.channels = c1
a1.sinks = sk1

a1.sources.s1.type = TAILDIR
a1.sources.s1.positionFile = /opt/datas/flume/taildir_position.json
a1.sources.s1.filegroups = f1 f2
a1.sources.s1.filegroups.f1 = /usr/local/nginx/logs/log/access.log
a1.sources.s1.filegroups.f2 = /usr/local/nginx/logs/log/.*.log.*

a1.sinks.sk1.type = hdfs
a1.sinks.sk1.hdfs.path=hdfs://mingwang-1:8020/flume/%y-%m-%d/%H-%M/
a1.sinks.sk1.hdfs.filePrefix= baidu-
a1.sinks.sk1.hdfs.round = true
a1.sinks.sk1.hdfs.roundValue = 10
a1.sinks.sk1.hdfs.roundUnit = minute
a1.sinks.sk1.hdfs.rollInterval= 0
a1.sinks.sk1.hdfs.rollSize= 6114
a1.sinks.sk1.hdfs.rollCount= 0
a1.sinks.sk1.hdfs.batchSize = 1
a1.sinks.sk1.hdfs.idleTimeout= 60
a1.sinks.sk1.hdfs.useLocalTimeStamp = true
a1.sinks.sk1.hdfs.fileType = DataStream

a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

a1.sources.s1.channels = c1
a1.sinks.sk1.channel = c1
'''

启动命令：
flume-ng agent -n a1 -c conf -f /opt/datas/flume/aa.conf

## sqoop配置
mv sqoop-env-template.sh sqoop-env.sh
vim sqoop-env.sh

export HADOOP_COMMON_HOME=/opt/hadoop
export HADOOP_MAPRED_HOME=/opt/hadoop
export HIVE_HOME=/opt/hive
export HIVE_CONF_DIR=/opt/hive/conf

如果报zookeeper的错误把他们注释了就可以了 vim bin/configure-sqoop

--------------------测试连通性-----------------------
sqoop list-tables -connect jdbc:mysql://mingwang-1:3306/hive --username root --password root

--------------------问题解决-----------------------
将hive的lib里面的拷贝到sqoop的lib目录下
cp hive-exec-1.2.1.jar /usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib/

--------------------找不到主类-----------------
cd /opt/sqoop/lib目录下
cp sqoop-1.4.4.jar /opt/hadoop/share/hadoop/mapreduce/lib/

----------------------报错解决-----------------------------
cp /opt/hive/lib/hive-shims-* /opt/sqoop/lib/

----------------------测试数据库到hive-----------------
编写脚本文件 vi conf1
