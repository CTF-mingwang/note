# hadoop，flume（1.9.0支持它TAILDIR），hive，sqoop

## 环境变量
export JAVA_HOME=/opt/jdk
export PATH=:$JAVA_HOME/bin:$PATH

export TOMCAT_HOME=/opt/tomcat
export PATH=:$TOMCAT_HOME/bin:$PATH

export HADOOP_HOME=/opt/hadoop
export PATH=:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH

export HIVE_HOME=/opt/hive
export PATH=:$HIVE_HOME/bin:$PATH

export SQOOP_HOME=/opt/sqoop
export PATH=:$SQOOP_HOME/bin:$PATH

export FLUME_HOME=/opt/flume
export PATH=:$FLUME_HOME/bin:$PATH

## hadoop
mapred-site.xml  可能没有，但是有一个模板文件（执行命令：cp mapred-site.xml.template mapred-site.xml）
<configuration>
        <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
        </property>
</configuration>

core-site.xml          mingwang-1是我的主机名，我在/etc/hosts里做了映射，这里可以换成本机ip或127.0.0.1
<configuration>
 <property>
   <name>fs.default.name</name>
   <value>hdfs://mingwang-1:8020</value>
 </property>
</configuration>

hdfs-site.xml    指定副本数、namenode和datanode的存储位置
<configuration>
 <property>
   <name>dfs.replication</name>
   <value>1</value>
 </property>
  <property>
     <name>dfs.namenode.name.dir</name>
     <value>/home/hadoop/dfs/name</value>
  </property>
  <property>
     <name>dfs.datanode.data.dir</name>
     <value>/home/hadoop/dfs/data</value>
  </property>
</configuration>

yarn-site.xml 
<configuration>
 <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
 </property>
</configuration>

slaves 这个是写datanode的ip地址的，由于是单节点的，所以写本机就行了
localhost

hdfs-site.xml  修改一下java的家目录，和hadoop的配文件目录

### 启动
hadoop namenode -format
hadoop-daemon.sh start namenode
hadoop-daemon.sh start datanode
start-yarn.sh

## hive

hive-env.sh
cp hive-env.sh.template hive-env.sh
# 相应的目录换成自己的
# hadoop 目录
HADOOP_HOME=/opt/hadoop

# hive 配置目录
export HIVE_CONF_DIR=/opt/hive/conf

# hive 的lib目录
export HIVE_AUX_JARS_PATH=/opt/hive/lib

hive-site.xml
<configuration>
     <!-- jdbc-url -->
     <property>
         <name>javax.jdo.option.ConnectionURL</name>
         <value>jdbc:mysql://mingwang-1:3306/hive?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
     </property>
     <!-- jdbc-driver -->
     <property>
         <name>javax.jdo.option.ConnectionDriverName</name>
         <value>com.mysql.jdbc.Driver</value>
     </property>
     <!-- 数据库用户名 -->
     <property>
         <name>javax.jdo.option.ConnectionUserName</name>
         <value>root</value>
     </property>
     <!-- 数据密码-->
     <property>
         <name>javax.jdo.option.ConnectionPassword</name>
         <value>root</value>
     </property>
     <property>
         <name>datanucleus.readOnlyDatastore</name>
         <value>false</value>
     </property>
     <property>
         <name>datanucleus.fixedDatastore</name>
         <value>false</value>
     </property>
     <property>
         <name>datanucleus.autoCreateSchema</name>
         <value>true</value>
     </property>
     <property>
         <name>datanucleus.schema.autoCreateAll</name>
         <value>true</value>
     </property>
     <property>
         <name>datanucleus.autoCreateTables</name>
         <value>true</value>
     </property>
     <property>
         <name>datanucleus.autoCreateColumns</name>
         <value>true</value>
     </property>
     <property>
         <name>hive.metastore.local</name>
         <value>true</value>
     </property>
     <!-- 显示表的列名 -->
     <property>
         <name>hive.cli.print.header</name>
         <value>true</value>
     </property>
     <!-- 显示数据库名称 -->
     <property>
         <name>hive.cli.print.current.db</name>
         <value>true</value>
     </property>
     <property>
         <name>hive.exec.local.scratchdir</name>
         <value>/usr/local/hive</value>
         <description>Local scratch space for Hive jobs</description>
     </property>
</configuration>

# 格式化数据库
 schematool -dbType mysql -initSchema
 
 # 启动hive
 hive
 
 # 查看数据库
 hive> show databases;

## flume
cd /opt/flume/conf
cp flume-env.sh.template flume-env.sh
vim flume-env.sh
#增加JAVA_HOME
export JAVA_HOME=/opt/jdk
执行的文件自己配置
我把他们放到/opt/datas/flume 里了
'''conf
a1.sources = s1
a1.channels = c1
a1.sinks = sk1

a1.sources.s1.type = TAILDIR
a1.sources.s1.positionFile = /opt/datas/flume/taildir_position.json
a1.sources.s1.filegroups = f1 f2
a1.sources.s1.filegroups.f1 = /usr/local/nginx/logs/log/access.log
a1.sources.s1.filegroups.f2 = /usr/local/nginx/logs/log/.*.log.*

a1.sinks.sk1.type = hdfs
a1.sinks.sk1.hdfs.path=hdfs://mingwang-1:8020/flume/%y-%m-%d/%H-%M/
a1.sinks.sk1.hdfs.filePrefix= baidu-
a1.sinks.sk1.hdfs.round = true
a1.sinks.sk1.hdfs.roundValue = 10
a1.sinks.sk1.hdfs.roundUnit = minute
a1.sinks.sk1.hdfs.rollInterval= 0
a1.sinks.sk1.hdfs.rollSize= 6114
a1.sinks.sk1.hdfs.rollCount= 0
a1.sinks.sk1.hdfs.batchSize = 1
a1.sinks.sk1.hdfs.idleTimeout= 60
a1.sinks.sk1.hdfs.useLocalTimeStamp = true
a1.sinks.sk1.hdfs.fileType = DataStream

a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

a1.sources.s1.channels = c1
a1.sinks.sk1.channel = c1
'''

启动命令：
flume-ng agent -n a1 -c conf -f /opt/datas/flume/aa.conf

## sqoop配置
mv sqoop-env-template.sh sqoop-env.sh
vim sqoop-env.sh

export HADOOP_COMMON_HOME=/opt/hadoop
export HADOOP_MAPRED_HOME=/opt/hadoop
export HIVE_HOME=/opt/hive
export HIVE_CONF_DIR=/opt/hive/conf

如果报zookeeper的错误把他们注释了就可以了 vim configure-sqoop

--------------------测试连通性-----------------------
sqoop list-tables -connect jdbc:mysql://mingwang-1:3306/hive --username root --password root

--------------------问题解决-----------------------
将hive的lib里面的拷贝到sqoop的lib目录下
cp hive-exec-1.2.1.jar /usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib/

--------------------找不到主类-----------------
cd /opt/sqoop/lib目录下
cp sqoop-1.4.4.jar /opt/hadoop/share/hadoop/mapreduce/lib/

----------------------报错解决-----------------------------
cp /opt/hive/lib/hive-shims-* /opt/sqoop/lib/

----------------------测试数据库到hive-----------------
编写脚本文件 vi conf1
